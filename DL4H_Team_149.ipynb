{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Public Github: https://github.com/kchand12/DL4H_Team_149/tree/main\n",
        "\n",
        "Author: Kanjana Chandren (kanjana3@illinois.edu)"
      ],
      "metadata": {
        "id": "c2xI1I5hxp9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * The paper focuses on predictive modeling using deep learning to predicut ICU length of stay (LoS). The problem involves feature engineering and data processing to address issues in EHR data.\n",
        "  * Accurate prediction of ICU LoS is important for hospital resource management to estimate staffing, bed allocation and resource optimization for better patient care quality.\n",
        "  * The problem is challenging due to the nature of ICU data which is sparse and irregularly sampled. The variability in patient conditions and treamtment responses makes standardization and accuract prediction hard.\n",
        "  * Prior methods include Long Short-Term Memory(LSTM) networks however there were challenges in handling ICU data specifically the models need for real-time updates and managing data\n",
        "*   Paper explanation\n",
        "  * The paper proposes a new model, the Temporal Pointwise Convolution (TPC) network, that addresses the challenges of ICU LoS prediction more effectively than existing methods.\n",
        "  * The TPC model combines temporal convolutional layers to capture time-dependent patterns and pointwise convolutional layers to extract features from complex interactions in patient data. This combination is works well at processing irregular and skewed data typical of ICU records.\n",
        "  * The Temporal Pointwise Convolution (TPC) model significantly outperformed other baseline models, achieving a Mean Absolute Deviation (MAD) of 1.55 days on the eICU dataset and 2.28 days on the MIMIC-IV dataset, and reducing the Mean Squared Logarithmic Error (MSLE) to 0.70 and 0.39, respectively.\n",
        "  * The introduction of the TPC model represents a significant advancement in predictive modeling for ICU management. It addresses specific data challenges in an ICU setting and outperforms existing models. This work contributes to both the theoretical and practical aspects of patient care optimization.\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: The TPC model will outperform traditional deep learning models like Long Short-Term Memory and Transformer models in accurately predicting the remaining length of stay for patients in the ICU.\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ],
      "metadata": {
        "id": "LM4WUjz64C3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(img_dir)\n",
        "cv2.imshow(\"Title\", img)\n"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import argspace\n",
        "import json\n",
        "import os\n",
        "from itertools import islice\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: The data is from the eICU Collaborative Research Database, which is an intensive care unit database available by Philips Healthcare in partnership with the MIT Laboratory for Computational Physiology. This dataset includes a variety of de-identified data from over 200,000 ICU admissions across the United States. The dataset can be accessed from PhysioNet, and accessing this data requires approval after agreeing to a specified use agreement.\n",
        "  * Statistics: For modeling purposes, the dataset is split 70% for training, 15% for validation, and 15% for testing.\n",
        "  * Data process: Data from various tables like patient, lab, nurseCharting and processed to create feature. The data is organized into:\n",
        "    * Time-series data that continously measure variables like heart rate and blood pressure.\n",
        "    * Diagnoses data from diagnostic codes during ICU stay.\n",
        "    * Flat features which are static information about patient demographics and detsils.\n",
        "  Labels are created for clinical outcomes like mortality, duration of stay and remission. The data is also split into training, validation and testing so it can generalize well. Lastly all continous variables are normazlied for model training.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/eICU_data'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  paths = {\n",
        "        'diagnoses': f'{raw_data_dir}/diagnoses.csv',\n",
        "        'flat_features': f'{raw_data_dir}/flat_features.csv',\n",
        "        'labels': f'{raw_data_dir}/labels.csv',\n",
        "        'timeseries_aperiodic': f'{raw_data_dir}/timeseriesaperiodic.csv',\n",
        "        'timeseries_lab': f'{raw_data_dir}/timeserieslab.csv',\n",
        "        'timeseries_nurse': f'{raw_data_dir}/timeseriesnurse.csv',\n",
        "        'timeseries_periodic': f'{raw_data_dir}/timeseriesperiodic.csv',\n",
        "        'timeseries_resp': f'{raw_data_dir}/timeseriesresp.csv'\n",
        "    }\n",
        "\n",
        "  data = {key: pd.read_csv(path) for key, path in paths.items()}\n",
        "\n",
        "  return data\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This section is processing the timeseries data\n",
        "def reconfigure_timeseries(timeseries, offset_column, feature_column=None, test=False):\n",
        "    if test:\n",
        "        timeseries = timeseries.iloc[300000:5000000]\n",
        "    timeseries.set_index(['patientunitstayid', pd.to_timedelta(timeseries[offset_column], unit='T')], inplace=True)\n",
        "    timeseries.drop(columns=offset_column, inplace=True)\n",
        "    if feature_column is not None:\n",
        "        timeseries = timeseries.pivot_table(columns=feature_column, index=timeseries.index)\n",
        "    timeseries.index = pd.MultiIndex.from_tuples(timeseries.index, names=['patient', 'time'])\n",
        "    return timeseries\n",
        "\n",
        "def resample_and_mask(timeseries, eICU_path, header, mask_decay=True, decay_rate=4/3, test=False,\n",
        "                       verbose=False, length_limit=24*14):\n",
        "    if test:\n",
        "        mask_decay = False\n",
        "        verbose = True\n",
        "    if verbose:\n",
        "        print('Resampling to 1 hour intervals...')\n",
        "    # take the mean of any duplicate index entries for unstacking\n",
        "    timeseries = timeseries.groupby(level=[0, 1]).mean()\n",
        "\n",
        "    timeseries.reset_index(level=1, inplace=True)\n",
        "    timeseries.time = timeseries.time.dt.ceil(freq='H')\n",
        "    timeseries.set_index('time', append=True, inplace=True)\n",
        "    timeseries.reset_index(level=0, inplace=True)\n",
        "    resampled = timeseries.groupby('patient').resample('H', closed='right', label='right').mean().drop(columns='patient')\n",
        "    del (timeseries)\n",
        "\n",
        "    def apply_mask_decay(mask_bool):\n",
        "        mask = mask_bool.astype(int)\n",
        "        mask.replace({0: np.nan}, inplace=True)  # so that forward fill works\n",
        "        inv_mask_bool = ~mask_bool\n",
        "        count_non_measurements = inv_mask_bool.cumsum() - \\\n",
        "                                 inv_mask_bool.cumsum().where(mask_bool).ffill().fillna(0)\n",
        "        decay_mask = mask.ffill().fillna(0) / (count_non_measurements * decay_rate).replace(0, 1)\n",
        "        return decay_mask\n",
        "\n",
        "    # store which values had to be imputed\n",
        "    if mask_decay:\n",
        "        if verbose:\n",
        "            print('Calculating mask decay features...')\n",
        "        mask_bool = resampled.notnull()\n",
        "        mask = mask_bool.groupby('patient').transform(apply_mask_decay)\n",
        "        del (mask_bool)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print('Calculating binary mask features...')\n",
        "        mask = resampled.notnull()\n",
        "        mask = mask.astype(int)\n",
        "\n",
        "    if verbose:\n",
        "        print('Filling missing data forwards...')\n",
        "    # carry forward missing values (note they will still be 0 in the nulls table)\n",
        "    resampled = resampled.fillna(method='ffill')\n",
        "\n",
        "    # simplify the indexes of both tables\n",
        "    mask = mask.rename(index=dict(zip(mask.index.levels[1],\n",
        "                                      mask.index.levels[1].days*24 + mask.index.levels[1].seconds//3600)))\n",
        "    resampled = resampled.rename(index=dict(zip(resampled.index.levels[1],\n",
        "                                                resampled.index.levels[1].days*24 +\n",
        "                                                resampled.index.levels[1].seconds//3600)))\n",
        "\n",
        "    # clip to length_limit\n",
        "    if length_limit is not None:\n",
        "        within_length_limit = resampled.index.get_level_values(1) < length_limit\n",
        "        resampled = resampled.loc[within_length_limit]\n",
        "        mask = mask.loc[within_length_limit]\n",
        "\n",
        "    if verbose:\n",
        "        print('Filling in remaining values with zeros...')\n",
        "    resampled.fillna(0, inplace=True)\n",
        "\n",
        "    # rename the columns in pandas for the mask so it doesn't complain\n",
        "    mask.columns = [str(col) + '_mask' for col in mask.columns]\n",
        "\n",
        "    # merge the mask with the features\n",
        "    final = pd.concat([resampled, mask], axis=1)\n",
        "    final.reset_index(level=1, inplace=True)\n",
        "    final = final.loc[final.time > 0]\n",
        "\n",
        "    if verbose:\n",
        "        print('Saving progress...')\n",
        "    # save to csv\n",
        "    if test is False:\n",
        "        final.to_csv(eICU_path + 'preprocessed_timeseries.csv', mode='a', header=header)\n",
        "    return\n",
        "\n",
        "def gen_patient_chunk(patients, size=1000):\n",
        "    it = iter(patients)\n",
        "    chunk = list(islice(it, size))\n",
        "    while chunk:\n",
        "        yield chunk\n",
        "        chunk = list(islice(it, size))\n",
        "\n",
        "def gen_timeseries_file(eICU_path, test=False):\n",
        "    print('==> Loading data from timeseries files...')\n",
        "    if test:\n",
        "        # Load a subset of each file for testing purposes\n",
        "        timeseries_lab = raw_data['timeseries_lab'].iloc[:500000]\n",
        "        timeseries_resp = raw_data['timeseries_resp'].iloc[:500000]\n",
        "        timeseries_nurse = raw_data['timeseries_nurse'].iloc[:500000]\n",
        "        timeseries_periodic = raw_data['timeseries_periodic'].iloc[:500000]\n",
        "        timeseries_aperiodic = raw_data['timeseries_aperiodic'].iloc[:500000]\n",
        "    else:\n",
        "        timeseries_lab = raw_data['timeseries_lab']\n",
        "        timeseries_resp = raw_data['timeseries_resp']\n",
        "        timeseries_nurse = raw_data['timeseries_nurse']\n",
        "        timeseries_periodic = raw_data['timeseries_periodic']\n",
        "        timeseries_aperiodic = raw_data['timeseries_aperiodic']\n",
        "\n",
        "    # Continue with data reconfiguration and further processing...\n",
        "    print('==> Reconfiguring lab timeseries...')\n",
        "    timeseries_lab = reconfigure_timeseries(timeseries_lab, offset_column='labresultoffset', feature_column='labname', test=test)\n",
        "    timeseries_lab.columns = [col.split('_')[1] if '_' in col else col for col in timeseries_lab.columns]  # Simplifying column names if necessary\n",
        "    print('==> Reconfiguring respiratory timeseries...')\n",
        "    # get rid of % signs (found in FiO2 section) and then convert into numbers\n",
        "    timeseries_resp = timeseries_resp.replace('%', '', regex=True)\n",
        "    timeseries_resp['respchartvalue'] = pd.to_numeric(timeseries_resp['respchartvalue'], errors='coerce')\n",
        "    timeseries_resp = timeseries_resp.loc[timeseries_resp['respchartvalue'].notnull()]\n",
        "    timeseries_resp = reconfigure_timeseries(timeseries_resp,\n",
        "                                             offset_column='respchartoffset',\n",
        "                                             feature_column='respchartvaluelabel',\n",
        "                                             test=test)\n",
        "    timeseries_resp.columns = timeseries_resp.columns.droplevel()\n",
        "\n",
        "    print('==> Reconfiguring nurse timeseries...')\n",
        "    # remove non numeric data\n",
        "    timeseries_nurse['nursingchartvalue'] = pd.to_numeric(timeseries_nurse['nursingchartvalue'], errors='coerce')\n",
        "    timeseries_nurse = timeseries_nurse.loc[timeseries_nurse['nursingchartvalue'].notnull()]\n",
        "    timeseries_nurse = reconfigure_timeseries(timeseries_nurse,\n",
        "                                              offset_column='nursingchartoffset',\n",
        "                                              feature_column='nursingchartcelltypevallabel',\n",
        "                                              test=test)\n",
        "    timeseries_nurse.columns = timeseries_nurse.columns.droplevel()\n",
        "\n",
        "    print('==> Reconfiguring aperiodic timeseries...')\n",
        "    timeseries_aperiodic = reconfigure_timeseries(timeseries_aperiodic,\n",
        "                                                  offset_column='observationoffset',\n",
        "                                                  test=test)\n",
        "\n",
        "    print('==> Reconfiguring periodic timeseries...')\n",
        "    timeseries_periodic = reconfigure_timeseries(timeseries_periodic,\n",
        "                                                 offset_column='observationoffset',\n",
        "                                                 test=test)\n",
        "\n",
        "    patients = timeseries_periodic.index.unique(level=0)\n",
        "\n",
        "    size = 4000\n",
        "    gen_chunks = gen_patient_chunk(patients, size=size)\n",
        "    i = size\n",
        "    header = True  # for the first chunk include the header in the csv file\n",
        "\n",
        "    print('==> Starting main processing loop...')\n",
        "\n",
        "    for patient_chunk in gen_chunks:\n",
        "\n",
        "        merged = timeseries_lab.loc[patient_chunk].append(timeseries_resp.loc[patient_chunk], sort=False)\n",
        "        merged = merged.append(timeseries_nurse.loc[patient_chunk], sort=False)\n",
        "        merged = merged.append(timeseries_periodic.loc[patient_chunk], sort=False)\n",
        "        merged = merged.append(timeseries_aperiodic.loc[patient_chunk], sort=True)\n",
        "\n",
        "        if i == size:  # fixed from first run\n",
        "            # all if not all are not normally distributed\n",
        "            quantiles = merged.quantile([0.05, 0.95])\n",
        "            maxs = quantiles.loc[0.95]\n",
        "            mins = quantiles.loc[0.05]\n",
        "\n",
        "        merged = 2 * (merged - mins) / (maxs - mins) - 1\n",
        "\n",
        "        # we then need to make sure that ridiculous outliers are clipped to something sensible\n",
        "        merged.clip(lower=-4, upper=4, inplace=True)  # room for +- 3 on each side, as variables are scaled roughly between 0 and 1\n",
        "\n",
        "        resample_and_mask(merged, eICU_path, header, mask_decay=True, decay_rate=4/3, test=test, verbose=False)\n",
        "        print('==> Processed ' + str(i) + ' patients...')\n",
        "        i += size\n",
        "        header = False\n",
        "\n",
        "    return\n",
        "def add_time_of_day(processed_timeseries, flat_features):\n",
        "\n",
        "    print('==> Adding time of day features...')\n",
        "    processed_timeseries = processed_timeseries.join(flat_features[['hour']], how='inner', on='patient')\n",
        "    processed_timeseries['hour'] = processed_timeseries['time'] + processed_timeseries['hour']\n",
        "    hour_list = np.linspace(0, 1, 24)  # make sure it's still scaled well\n",
        "    processed_timeseries['hour'] = processed_timeseries['hour'].apply(lambda x: hour_list[x%24 - 24])\n",
        "    return processed_timeseries\n",
        "\n",
        "def further_processing(eICU_path, test=False):\n",
        "\n",
        "    if test:\n",
        "        processed_timeseries = pd.read_csv(eICU_path + 'preprocessed_timeseries.csv', nrows=999999)\n",
        "    else:\n",
        "        processed_timeseries = pd.read_csv(eICU_path + 'preprocessed_timeseries.csv')\n",
        "    processed_timeseries.rename(columns={'Unnamed: 1': 'time'}, inplace=True)\n",
        "    processed_timeseries.set_index('patient', inplace=True)\n",
        "    flat_features = pd.read_csv(eICU_path + 'flat_features.csv')\n",
        "    flat_features.rename(columns={'patientunitstayid': 'patient'}, inplace=True)\n",
        "    processed_timeseries.sort_values(['patient', 'time'], inplace=True)\n",
        "    flat_features.set_index('patient', inplace=True)\n",
        "\n",
        "    processed_timeseries = add_time_of_day(processed_timeseries, flat_features)\n",
        "\n",
        "    if test is False:\n",
        "        print('==> Saving finalised preprocessed timeseries...')\n",
        "        # this will replace old one that was updated earlier in the script\n",
        "        processed_timeseries.to_csv(eICU_path + 'preprocessed_timeseries.csv')\n",
        "\n",
        "    return\n",
        "\n",
        "def timeseries_main(eICU_path, test=False):\n",
        "    # make sure the preprocessed_timeseries.csv file is not there because the first section of this script appends to it\n",
        "    if test is False:\n",
        "        print('==> Removing the preprocessed_timeseries.csv file if it exists...')\n",
        "        try:\n",
        "            os.remove(eICU_path + 'preprocessed_timeseries.csv')\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "    gen_timeseries_file(eICU_path, test)\n",
        "    further_processing(eICU_path, test)\n",
        "    return"
      ],
      "metadata": {
        "id": "9rnFqgDiYjwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper functions that process diagnoses data\n",
        "def add_codes(splits, codes_dict, words_dict, count):\n",
        "    codes = list()\n",
        "    levels = len(splits)  # the max number of levels is 6\n",
        "    if levels >= 1:\n",
        "        try:\n",
        "            codes.append(codes_dict[splits[0]][0])\n",
        "            codes_dict[splits[0]][2] += 1\n",
        "        except KeyError:\n",
        "            codes_dict[splits[0]] = [count, {}, 0]\n",
        "            codes.append(count)\n",
        "            words_dict[count] = splits[0]\n",
        "            count += 1\n",
        "    if levels >= 2:\n",
        "        try:\n",
        "            codes.append(codes_dict[splits[0]][1][splits[1]][0])\n",
        "            codes_dict[splits[0]][1][splits[1]][2] += 1\n",
        "        except KeyError:\n",
        "            codes_dict[splits[0]][1][splits[1]] = [count, {}, 0]\n",
        "            codes.append(count)\n",
        "            words_dict[count] = splits[0] + '|' + splits[1]\n",
        "            count += 1\n",
        "    if levels >= 3:\n",
        "        try:\n",
        "            codes.append(codes_dict[splits[0]][1][splits[1]][1][splits[2]][0])\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]][2] += 1\n",
        "        except KeyError:\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]] = [count, {}, 0]\n",
        "            codes.append(count)\n",
        "            words_dict[count] = splits[0] + '|' + splits[1] + '|' + splits[2]\n",
        "            count += 1\n",
        "    if levels >= 4:\n",
        "        try:\n",
        "            codes.append(codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][0])\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][2] += 1\n",
        "        except KeyError:\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]] = [count, {}, 0]\n",
        "            codes.append(count)\n",
        "            words_dict[count] = splits[0] + '|' + splits[1] + '|' + splits[2] + '|' + splits[3]\n",
        "            count += 1\n",
        "    if levels >= 5:\n",
        "        try:\n",
        "            codes.append(codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][1][splits[4]][0])\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][1][splits[4]][2] += 1\n",
        "        except KeyError:\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][1][splits[4]] = [count, {}, 0]\n",
        "            codes.append(count)\n",
        "            words_dict[count] = splits[0] + '|' + splits[1] + '|' + splits[2] + '|' + splits[3] + '|' + splits[4]\n",
        "            count += 1\n",
        "    if levels is 6:\n",
        "        try:\n",
        "            codes.append(codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][1][splits[4]][1][splits[5]][0])\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][1][splits[4]][1][splits[5]][2] += 1\n",
        "        except KeyError:\n",
        "            codes_dict[splits[0]][1][splits[1]][1][splits[2]][1][splits[3]][1][splits[4]][1][splits[5]] = [count, {}, 0]\n",
        "            codes.append(count)\n",
        "            words_dict[count] = splits[0] + '|' + splits[1] + '|' + splits[2] + '|' + splits[3] + '|' + splits[4] + '|' + splits[5]\n",
        "            count += 1\n",
        "    return codes, count\n",
        "\n",
        "\n",
        "def get_mapping_dict(unique_diagnoses):\n",
        "\n",
        "    # a lot of the notes strings look the same, so we will not propagate beyond Organ Systems for this:\n",
        "    main_diagnoses = [a for a in unique_diagnoses if not (a.startswith('notes') or a.startswith('admission'))]\n",
        "    adm_diagnoses = [a for a in unique_diagnoses if a.startswith('admission diagnosis')]\n",
        "    pasthistory_organsystems = [a for a in unique_diagnoses if a.startswith('notes/Progress Notes/Past History/Organ Systems/')]\n",
        "    pasthistory_comments = [a for a in unique_diagnoses if a.startswith('notes/Progress Notes/Past History/Past History Obtain Options')]\n",
        "\n",
        "    # sort into alphabetical order to keep the codes roughly together numerically.\n",
        "    main_diagnoses.sort()\n",
        "    adm_diagnoses.sort()\n",
        "    pasthistory_organsystems.sort()\n",
        "    pasthistory_comments.sort()\n",
        "\n",
        "    mapping_dict = {}\n",
        "    codes_dict = {}\n",
        "    words_dict = {}\n",
        "    count = 0\n",
        "\n",
        "    for diagnosis in main_diagnoses:\n",
        "        splits = diagnosis.split('|')\n",
        "        codes, count = add_codes(splits, codes_dict, words_dict, count)\n",
        "        # add all codes relevant to the diagnosisstring\n",
        "        mapping_dict[diagnosis] = codes\n",
        "\n",
        "    for diagnosis in adm_diagnoses:\n",
        "        # take out the things that are common to all of these because it creates unnecessary levels\n",
        "        shortened = diagnosis.replace('admission diagnosis|', '')\n",
        "        shortened = shortened.replace('All Diagnosis|', '')\n",
        "        shortened = shortened.replace('Additional APACHE  Information|', '')\n",
        "        splits = shortened.split('|')\n",
        "        codes, count = add_codes(splits, codes_dict, words_dict, count)\n",
        "        mapping_dict[diagnosis] = codes\n",
        "\n",
        "    for diagnosis in pasthistory_organsystems:\n",
        "        # take out the things that are common to all of these because it creates unnecessary levels\n",
        "        shortened = diagnosis.replace('notes/Progress Notes/Past History/Organ Systems/', '')\n",
        "        splits = shortened.split('/')  # note different split to main_diagnoses\n",
        "        codes, count = add_codes(splits, codes_dict, words_dict, count)\n",
        "        # add all codes relevant to the diagnosisstring\n",
        "        mapping_dict[diagnosis] = codes\n",
        "\n",
        "    for diagnosis in pasthistory_comments:\n",
        "        # take out the things that are common to all of these because it creates unnecessary levels\n",
        "        shortened = diagnosis.replace('notes/Progress Notes/Past History/Past History Obtain Options/', '')\n",
        "        splits = shortened.split('/')  # note different split to main_diagnoses\n",
        "        codes, count = add_codes(splits, codes_dict, words_dict, count)\n",
        "        # add all codes relevant to the diagnosisstring\n",
        "        mapping_dict[diagnosis] = codes\n",
        "\n",
        "    return codes_dict, mapping_dict, count, words_dict\n",
        "\n",
        "# get rid of anything that is a parent to only one child (index 2 is 1)\n",
        "def find_pointless_codes(diag_dict):\n",
        "    pointless_codes = []\n",
        "    for key, value in diag_dict.items():\n",
        "        # if there is only one child, then the branch is linear and can be condensed\n",
        "        if value[2] is 1:\n",
        "            pointless_codes.append(value[0])\n",
        "        # get rid of repeat copies where the parent and child are the same title\n",
        "        for next_key, next_value in value[1].items():\n",
        "            if key.lower() == next_key.lower():\n",
        "                pointless_codes.append(next_value[0])\n",
        "        pointless_codes += find_pointless_codes(value[1])\n",
        "    return pointless_codes\n",
        "\n",
        "# get rid of any codes that have a frequency of less than cut_off\n",
        "def find_rare_codes(cut_off, sparse_df):\n",
        "    prevalence = sparse_df.sum(axis=0)  # see if you can stop it making pointless extra classes\n",
        "    rare_codes = prevalence.loc[prevalence <= cut_off].index\n",
        "    return list(rare_codes)\n",
        "\n",
        "def add_apache_diag(sparse_df, raw_data, cut_off):\n",
        "    print('==> Adding admission diagnoses from flat_features...')\n",
        "    flat = raw_data['flat_features']\n",
        "    adm_diag = flat[['patientunitstayid', 'apacheadmissiondx']]\n",
        "    adm_diag.set_index('patientunitstayid', inplace=True)\n",
        "    adm_diag = pd.get_dummies(adm_diag, columns=['apacheadmissiondx'])\n",
        "\n",
        "    rare_adm_diag = find_rare_codes(cut_off, adm_diag)\n",
        "    groupby_dict = {}\n",
        "    for diag in adm_diag.columns:\n",
        "        if diag in rare_adm_diag:\n",
        "            groupby_dict[diag] = 'groupedapacheadmissiondx_' + diag.split(' ', 1)[0].split('/', 1)[0].split(',', 1)[0][18:]\n",
        "        else:\n",
        "            groupby_dict[diag] = diag\n",
        "    adm_diag = adm_diag.groupby(groupby_dict, axis=1).sum()\n",
        "    rare_adm_diag = find_rare_codes(cut_off, adm_diag)\n",
        "    adm_diag.drop(columns=rare_adm_diag, inplace=True)\n",
        "    all_diag = sparse_df.join(adm_diag, how='outer', on='patientunitstayid')\n",
        "    return all_diag\n",
        "\n",
        "def diagnoses_main(eICU_path, raw_data, cut_off_prevalence):\n",
        "    print('==> Analyzing and processing diagnoses data...')\n",
        "    diagnoses = raw_data['diagnoses']\n",
        "    diagnoses.set_index('patientunitstayid', inplace=True)\n",
        "\n",
        "    unique_diagnoses = diagnoses.diagnosisstring.unique()\n",
        "    codes_dict, mapping_dict, count, words_dict = get_mapping_dict(unique_diagnoses)\n",
        "\n",
        "    patients = diagnoses.index.unique()\n",
        "    index_to_patients = dict(enumerate(patients))\n",
        "    patients_to_index = {v: k for k, v in index_to_patients.items()}\n",
        "\n",
        "    diagnoses = diagnoses.groupby('patientunitstayid').apply(lambda diag: diag.to_dict(orient='list')['diagnosisstring']).to_dict()\n",
        "    diagnoses = {patient: [code for diag in list_diag for code in mapping_dict[diag]] for (patient, list_diag) in diagnoses.items()}\n",
        "\n",
        "    num_patients = len(patients)\n",
        "    sparse_diagnoses = np.zeros((num_patients, count))\n",
        "    for patient, codes in diagnoses.items():\n",
        "        sparse_diagnoses[patients_to_index[patient], codes] = 1\n",
        "\n",
        "    sparse_df = pd.DataFrame(sparse_diagnoses, index=patients, columns=range(count))\n",
        "    cut_off = round(cut_off_prevalence * num_patients)\n",
        "    sparse_df = add_apache_diag(sparse_df, raw_data, cut_off)\n",
        "\n",
        "    sparse_df.rename(columns=words_dict, inplace=True)\n",
        "    print('==> Finalizing and saving processed diagnoses data...')\n",
        "    sparse_df.to_csv(eICU_path + 'preprocessed_diagnoses.csv')\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "HaSf08YFeSCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper function to preprocess flat features\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def preprocess_flat(flat):\n",
        "\n",
        "    # make naming consistent with the other tables\n",
        "    flat.rename(columns={'patientunitstayid': 'patient'}, inplace=True)\n",
        "    flat.set_index('patient', inplace=True)\n",
        "\n",
        "    # admission diagnosis is dealt with in diagnoses.py not flat features\n",
        "    flat.drop(columns=['apacheadmissiondx'], inplace=True)\n",
        "\n",
        "    # drop apache variables as these aren't available until 24 hours into the stay\n",
        "    flat.drop(columns=['eyes', 'motor', 'verbal', 'dialysis', 'vent', 'meds', 'intubated', 'bedcount'], inplace=True)\n",
        "\n",
        "    flat['gender'].replace({'Male': 1, 'Female': 0}, inplace=True)\n",
        "    flat['teachingstatus'].replace({'t': 1, 'f': 0}, inplace=True)\n",
        "\n",
        "    cat_features = ['ethnicity', 'unittype', 'unitadmitsource', 'unitvisitnumber', 'unitstaytype',\n",
        "                                         'physicianspeciality', 'numbedscategory', 'region']\n",
        "    # get rid of any really uncommon values\n",
        "    for f in cat_features:\n",
        "        too_rare = [value for value, count in flat[f].value_counts().iteritems() if count < 1000]\n",
        "        flat.loc[flat[f].isin(too_rare), f] = 'misc'\n",
        "\n",
        "    # convert the categorical features to one-hot\n",
        "    flat = pd.get_dummies(flat, columns=cat_features)\n",
        "\n",
        "    # 10 patients have NaN for age; we fill this with the mean value which is 63\n",
        "    flat['age'].fillna('63', inplace=True)\n",
        "    # some of the ages are like '> 89' rather than numbers, this needs removing and converting to numbers\n",
        "    # but we make an extra variable to keep this information\n",
        "    flat['> 89'] = flat['age'].str.contains('> 89').astype(int)\n",
        "    flat['age'] = flat['age'].replace('> ', '', regex=True)\n",
        "    flat['age'] = [float(value) for value in flat.age.values]\n",
        "\n",
        "    # note that the features imported from the time series have already been normalised\n",
        "    # standardisation is for features that are probably normally distributed\n",
        "    features_for_standardisation = 'admissionheight'\n",
        "    means = flat[features_for_standardisation].mean(axis=0)\n",
        "    stds = flat[features_for_standardisation].std(axis=0)\n",
        "    flat[features_for_standardisation] = (flat[features_for_standardisation] - means) / stds\n",
        "\n",
        "    # probably not normally distributed\n",
        "    features_for_min_max = ['admissionweight', 'age', 'hour']#, 'bedcount']\n",
        "\n",
        "    def scale_min_max(flat):\n",
        "        quantiles = flat.quantile([0.05, 0.95])\n",
        "        maxs = quantiles.loc[0.95]\n",
        "        mins = quantiles.loc[0.05]\n",
        "        return 2 * (flat - mins) / (maxs - mins) - 1\n",
        "\n",
        "    flat[features_for_min_max] = flat[features_for_min_max].apply(scale_min_max)\n",
        "\n",
        "    # we then need to make sure that ridiculous outliers are clipped to something sensible\n",
        "    flat[features_for_standardisation] = flat[features_for_standardisation].clip(lower=-4, upper=4)  # room for +- 3 on each side of the normal range, as variables are scaled roughly between -1 and 1\n",
        "    flat[features_for_min_max] = flat[features_for_min_max].clip(lower=-4, upper=4)\n",
        "\n",
        "    # fill in the NaNs\n",
        "    # these are mainly found in admissionweight and admissionheight,\n",
        "    # so we create another variable to tell the model when this has been imputed\n",
        "    flat['nullweight'] = flat['admissionweight'].isnull().astype(int)\n",
        "    flat['nullheight'] = flat['admissionheight'].isnull().astype(int)\n",
        "    flat['admissionweight'].fillna(0, inplace=True)\n",
        "    flat['admissionheight'].fillna(0, inplace=True)\n",
        "    # there are only 11 missing genders but we might as well set this to 0.5 to tell the model we aren't sure\n",
        "    flat['gender'].fillna(0.5, inplace=True)\n",
        "    flat['gender'].replace({'Other': 0.5, 'Unknown': 0.5}, inplace=True)\n",
        "\n",
        "    return flat\n",
        "\n",
        "def preprocess_labels(labels):\n",
        "\n",
        "    # make naming consistent with the other tables\n",
        "    labels.rename(columns={'patientunitstayid': 'patient'}, inplace=True)\n",
        "    labels.set_index('patient', inplace=True)\n",
        "\n",
        "    labels = pd.get_dummies(labels, columns=['unitdischargelocation', 'unitdischargestatus'])\n",
        "\n",
        "    labels['actualhospitalmortality'].replace({'EXPIRED': 1, 'ALIVE': 0}, inplace=True)\n",
        "\n",
        "    return labels\n",
        "\n",
        "def flat_and_labels_main(raw_data, eICU_path):\n",
        "    print('==> Preprocessing flat features and labels...')\n",
        "    flat = preprocess_flat(raw_data['flat_features'])\n",
        "    labels = preprocess_labels(raw_data['labels'])\n",
        "\n",
        "    # Code to filter out any patients not present in timeseries data\n",
        "    try:\n",
        "        with open(eICU_path + 'stays.txt', 'r') as f:\n",
        "            ts_patients = [int(patient.rstrip()) for patient in f.readlines()]\n",
        "    except FileNotFoundError:\n",
        "        ts_patients = pd.read_csv(eICU_path + 'preprocessed_timeseries.csv')\n",
        "        ts_patients = [x for x in ts_patients.patient.unique()]\n",
        "        with open(eICU_path + 'stays.txt', 'w') as f:\n",
        "            for patient in ts_patients:\n",
        "                f.write(\"%s\\n\" % patient)\n",
        "\n",
        "    flat = flat.loc[ts_patients].copy()\n",
        "    labels = labels.loc[ts_patients].copy()\n",
        "\n",
        "    print('==> Saving finalised preprocessed labels and flat features...')\n",
        "    flat.to_csv(eICU_path + 'preprocessed_flat.csv')\n",
        "    labels.to_csv(eICU_path + 'preprocessed_labels.csv')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "B_54_OzohGZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting datasets into training and test\n",
        "def create_folder(parent_path, folder):\n",
        "    if not parent_path.endswith('/'):\n",
        "        parent_path += '/'\n",
        "    folder_path = parent_path + folder\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "    return folder_path\n",
        "\n",
        "def shuffle_stays(stays, seed=9):\n",
        "    return shuffle(stays, random_state=seed)\n",
        "\n",
        "def process_table(table_name, table, stays, folder_path):\n",
        "    table = table.loc[stays].copy()\n",
        "    table.to_csv('{}/{}.csv'.format(folder_path, table_name))\n",
        "    return\n",
        "\n",
        "def split_train_test(path, is_test=True, seed=9, cleanup=True, MIMIC=False):\n",
        "\n",
        "    labels = pd.read_csv(path + 'preprocessed_labels.csv')\n",
        "    labels.set_index('patient', inplace=True)\n",
        "    # we split by unique patient identifier to make sure there are no patients\n",
        "    # that cross into both the train and the test sets\n",
        "    patients = labels.uniquepid.unique()\n",
        "\n",
        "    train, test = train_test_split(patients, test_size=0.15, random_state=seed)\n",
        "    train, val = train_test_split(train, test_size=0.15/0.85, random_state=seed)\n",
        "\n",
        "    print('==> Loading data for splitting...')\n",
        "    if is_test:\n",
        "        timeseries = pd.read_csv(path + 'preprocessed_timeseries.csv', nrows=999999)\n",
        "    else:\n",
        "        timeseries = pd.read_csv(path + 'preprocessed_timeseries.csv')\n",
        "    timeseries.set_index('patient', inplace=True)\n",
        "    if not MIMIC:\n",
        "        diagnoses = pd.read_csv(path + 'preprocessed_diagnoses.csv')\n",
        "        diagnoses.set_index('patient', inplace=True)\n",
        "    flat_features = pd.read_csv(path + 'preprocessed_flat.csv')\n",
        "    flat_features.set_index('patient', inplace=True)\n",
        "\n",
        "    # delete the source files, as they won't be needed anymore\n",
        "    if is_test is False and cleanup:\n",
        "        print('==> Removing the unsorted data...')\n",
        "        os.remove(path + 'preprocessed_timeseries.csv')\n",
        "        if not MIMIC:\n",
        "            os.remove(path + 'preprocessed_diagnoses.csv')\n",
        "        os.remove(path + 'preprocessed_labels.csv')\n",
        "        os.remove(path + 'preprocessed_flat.csv')\n",
        "\n",
        "    for partition_name, partition in zip(['train', 'val', 'test'], [train, val, test]):\n",
        "        print('==> Preparing {} data...'.format(partition_name))\n",
        "        stays = labels.loc[labels['uniquepid'].isin(partition)].index\n",
        "        folder_path = create_folder(path, partition_name)\n",
        "        with open(folder_path + '/stays.txt', 'w') as f:\n",
        "            for stay in stays:\n",
        "                f.write(\"%s\\n\" % stay)\n",
        "        stays = shuffle_stays(stays, seed=9)\n",
        "        if MIMIC:\n",
        "            for table_name, table in zip(['labels', 'flat', 'timeseries'],\n",
        "                                         [labels, flat_features, timeseries]):\n",
        "                process_table(table_name, table, stays, folder_path)\n",
        "        else:\n",
        "            for table_name, table in zip(['labels', 'flat', 'diagnoses', 'timeseries'],\n",
        "                                         [labels, flat_features, diagnoses, timeseries]):\n",
        "                process_table(table_name, table, stays, folder_path)\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "ytAykb5XiRlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  stats={}\n",
        "  for key, df in raw_data.items():\n",
        "    stats[key]={\n",
        "        'rows':df.shape[0],\n",
        "        'columns':df.shape[1],\n",
        "        'descriptive_stats': df.describe()\n",
        "    }\n",
        "  return stats\n",
        "\n",
        "stats_data_train = calculate_stats(raw_data)\n",
        "print(\"Statistics for diagnoses:\")\n",
        "print(\"Number of rows:\", stats_data_train['diagnoses']['number_of_rows'])\n",
        "print(\"Number of columns:\", stats_data_train['diagnoses']['number_of_columns'])\n",
        "print(stats_data_train['diagnoses']['descriptive_statistics'])\n",
        "\n",
        "# main function that processes all the raw data\n",
        "def process_data():\n",
        "  # implement this function to process the data as you need\n",
        "  print('==> Removing the stays.txt file if it exists...')\n",
        "  try:\n",
        "    os.remove( + 'stays.txt')\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "  test = False\n",
        "  cut_off_prevalence = 0.01  # this would be 1%\n",
        "  gen_timeseries_file(raw_data_dir, test)\n",
        "  diagnoses_main(raw_data_dir, raw_data, cut_off_prevalence)\n",
        "  flat_and_labels_main(raw_data, raw_data_dir)\n",
        "  split_train_test(raw_data_dir, is_test=False)\n",
        "  return None\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "ArV0iZAPYVo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: There are 3 main layers with sub layers that include Conv1d and Linear layers. ReLu, Sigmoid and Hardtanh are used for non-linearities and manage output scales.\n",
        "  * Training objectives: The loss function using Mean Squared Log Error and Mean Squared Error combination for regression and Binary Cross Entropy for classification. Adam optimizer is used with a learning rate of 0.001 to handly sparse gradients. For multitask learning a weight alpha is used to balance loss components.\n",
        "  * Others: The model does not use pretrained weights and ther is no implementation of Monte Caro or other methods of uncertainty analysis.\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The below code is defining the TPC Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import cat, exp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import pad\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "\n",
        "###============== The main defining function of the TPC model is temp_pointwise() on line 403 ==============###\n",
        "\n",
        "\n",
        "# Mean Squared Logarithmic Error (MSLE) loss\n",
        "class MSLELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSLELoss, self).__init__()\n",
        "        self.squared_error = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, y_hat, y, mask, seq_length, sum_losses=False):\n",
        "        # the log(predictions) corresponding to no data should be set to 0\n",
        "        log_y_hat = y_hat.log().where(mask, torch.zeros_like(y))\n",
        "        # the we set the log(labels) that correspond to no data to be 0 as well\n",
        "        log_y = y.log().where(mask, torch.zeros_like(y))\n",
        "        # where there is no data log_y_hat = log_y = 0, so the squared error will be 0 in these places\n",
        "        loss = self.squared_error(log_y_hat, log_y)\n",
        "        loss = torch.sum(loss, dim=1)\n",
        "        if not sum_losses:\n",
        "            loss = loss / seq_length.clamp(min=1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# Mean Squared Error (MSE) loss\n",
        "class MSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSELoss, self).__init__()\n",
        "        self.squared_error = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, y_hat, y, mask, seq_length, sum_losses=False):\n",
        "        # the predictions corresponding to no data should be set to 0\n",
        "        y_hat = y_hat.where(mask, torch.zeros_like(y))\n",
        "        # the we set the labels that correspond to no data to be 0 as well\n",
        "        y = y.where(mask, torch.zeros_like(y))\n",
        "        # where there is no data log_y_hat = log_y = 0, so the squared error will be 0 in these places\n",
        "        loss = self.squared_error(y_hat, y)\n",
        "        loss = torch.sum(loss, dim=1)\n",
        "        if not sum_losses:\n",
        "            loss = loss / seq_length.clamp(min=1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class MyBatchNorm(_BatchNorm):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
        "                 track_running_stats=True):\n",
        "        super(MyBatchNorm, self).__init__(\n",
        "            num_features, eps, momentum, affine, track_running_stats)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self._check_input_dim(input)\n",
        "\n",
        "        # hack to work around model.eval() issue\n",
        "        if not self.training:\n",
        "            self.eval_momentum = 0  # set the momentum to zero when the model is validating\n",
        "\n",
        "        if self.momentum is None:\n",
        "            exponential_average_factor = 0.0\n",
        "        else:\n",
        "            exponential_average_factor = self.momentum if self.training else self.eval_momentum\n",
        "\n",
        "        if self.track_running_stats:\n",
        "            if self.num_batches_tracked is not None:\n",
        "                self.num_batches_tracked = self.num_batches_tracked + 1\n",
        "                if self.momentum is None:  # use cumulative moving average\n",
        "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
        "                else:  # use exponential moving average\n",
        "                    exponential_average_factor = self.momentum if self.training else self.eval_momentum\n",
        "\n",
        "        return F.batch_norm(\n",
        "            input, self.running_mean, self.running_var, self.weight, self.bias,\n",
        "            training=True, momentum=exponential_average_factor, eps=self.eps)  # set training to True so it calculates the norm of the batch\n",
        "\n",
        "\n",
        "class MyBatchNorm1d(MyBatchNorm):\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 2 and input.dim() != 3:\n",
        "            raise ValueError('expected 2D or 3D input (got {}D input)'.format(input.dim()))\n",
        "\n",
        "\n",
        "class EmptyModule(nn.Module):\n",
        "    def forward(self, X):\n",
        "        return X\n",
        "\n",
        "\n",
        "class TempPointConv(nn.Module):\n",
        "    def __init__(self, config, F=None, D=None, no_flat_features=None):\n",
        "\n",
        "        # The timeseries data will be of dimensions B * (2F + 2) * T where:\n",
        "        #   B is the batch size\n",
        "        #   F is the number of features for convolution (N.B. we start with 2F because there are corresponding mask features)\n",
        "        #   T is the number of timepoints\n",
        "        #   The other 2 features represent the sequence number and the hour in the day\n",
        "\n",
        "        # The diagnoses data will be of dimensions B * D where:\n",
        "        #   D is the number of diagnoses\n",
        "        # The flat data will be of dimensions B * no_flat_features\n",
        "\n",
        "        super(TempPointConv, self).__init__()\n",
        "        self.task = config.task\n",
        "        self.n_layers = config.n_layers\n",
        "        self.model_type = config.model_type\n",
        "        self.share_weights = config.share_weights\n",
        "        self.diagnosis_size = config.diagnosis_size\n",
        "        self.main_dropout_rate = config.main_dropout_rate\n",
        "        self.temp_dropout_rate = config.temp_dropout_rate\n",
        "        self.kernel_size = config.kernel_size\n",
        "        self.temp_kernels = config.temp_kernels\n",
        "        self.point_sizes = config.point_sizes\n",
        "        self.batchnorm = config.batchnorm\n",
        "        self.last_linear_size = config.last_linear_size\n",
        "        self.F = F\n",
        "        self.D = D\n",
        "        self.no_flat_features = no_flat_features\n",
        "        self.no_diag = config.no_diag\n",
        "        self.no_mask = config.no_mask\n",
        "        self.no_exp = config.no_exp\n",
        "        self.no_skip_connections = config.no_skip_connections\n",
        "        self.alpha = config.alpha\n",
        "        self.momentum = 0.01 if self.batchnorm == 'low_momentum' else 0.1\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.hardtanh = nn.Hardtanh(min_val=1/48, max_val=100)  # keep the end predictions between half an hour and 100 days\n",
        "        self.msle_loss = MSLELoss()\n",
        "        self.mse_loss = MSELoss()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "        self.main_dropout = nn.Dropout(p=self.main_dropout_rate)\n",
        "        self.temp_dropout = nn.Dropout(p=self.temp_dropout_rate)\n",
        "\n",
        "        self.remove_none = lambda x: tuple(xi for xi in x if xi is not None)  # removes None items from a tuple\n",
        "        self.empty_module = EmptyModule()\n",
        "\n",
        "        if self.batchnorm in ['mybatchnorm', 'pointonly', 'temponly', 'low_momentum']:\n",
        "            self.batchnormclass = MyBatchNorm1d\n",
        "        elif self.batchnorm == 'default':\n",
        "            self.batchnormclass = nn.BatchNorm1d\n",
        "\n",
        "        # input shape: B * D\n",
        "        # output shape: B * diagnosis_size\n",
        "        self.diagnosis_encoder = nn.Linear(in_features=self.D, out_features=self.diagnosis_size)\n",
        "\n",
        "        if self.batchnorm in ['mybatchnorm', 'pointonly', 'low_momentum', 'default']:\n",
        "            self.bn_diagnosis_encoder = self.batchnormclass(num_features=self.diagnosis_size, momentum=self.momentum)  # input shape: B * diagnosis_size\n",
        "            self.bn_point_last_los = self.batchnormclass(num_features=self.last_linear_size, momentum=self.momentum)  # input shape: (B * T) * last_linear_size\n",
        "            self.bn_point_last_mort = self.batchnormclass(num_features=self.last_linear_size, momentum=self.momentum)  # input shape: (B * T) * last_linear_size\n",
        "        else:\n",
        "            self.bn_diagnosis_encoder = self.empty_module\n",
        "            self.bn_point_last_los = self.empty_module\n",
        "            self.bn_point_last_mort = self.empty_module\n",
        "\n",
        "        # input shape: (B * T) * last_linear_size\n",
        "        # output shape: (B * T) * 1\n",
        "        self.point_final_los = nn.Linear(in_features=self.last_linear_size, out_features=1)\n",
        "        self.point_final_mort = nn.Linear(in_features=self.last_linear_size, out_features=1)\n",
        "\n",
        "        if self.model_type == 'tpc':\n",
        "            self.init_tpc()\n",
        "        elif self.model_type == 'temp_only':\n",
        "            self.init_temp()\n",
        "        elif self.model_type == 'pointwise_only':\n",
        "            self.init_pointwise()\n",
        "        else:\n",
        "            raise NotImplementedError('Specified model type not supported; supported types include tpc, temp_only and pointwise_only')\n",
        "\n",
        "\n",
        "    def init_tpc(self):\n",
        "\n",
        "        # non-module layer attributes\n",
        "        self.layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            dilation = i * (self.kernel_size - 1) if i > 0 else 1  # dilation = 1 for the first layer, after that it captures all the information gathered by previous layers\n",
        "            temp_k = self.temp_kernels[i]\n",
        "            point_size = self.point_sizes[i]\n",
        "            self.update_layer_info(layer=i, temp_k=temp_k, point_size=point_size, dilation=dilation, stride=1)\n",
        "\n",
        "        # module layer attributes\n",
        "        self.create_temp_pointwise_layers()\n",
        "\n",
        "        # input shape: (B * T) * ((F + Zt) * (1 + Y) + diagnosis_size + no_flat_features)\n",
        "        # output shape: (B * T) * last_linear_size\n",
        "        input_size = (self.F + self.Zt) * (1 + self.Y) + self.diagnosis_size + self.no_flat_features\n",
        "        if self.no_diag:\n",
        "            input_size = input_size - self.diagnosis_size\n",
        "        if self.no_skip_connections:\n",
        "            input_size = self.F * self.Y + self.Z + self.diagnosis_size + self.no_flat_features\n",
        "        self.point_last_los = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        self.point_last_mort = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def init_temp(self):\n",
        "\n",
        "        # non-module layer attributes\n",
        "        self.layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            dilation = i * (self.kernel_size - 1) if i > 0 else 1  # dilation = 1 for the first layer, after that it captures all the information gathered by previous layers\n",
        "            temp_k = self.temp_kernels[i]\n",
        "            self.update_layer_info(layer=i, temp_k=temp_k, dilation=dilation, stride=1)\n",
        "\n",
        "        # module layer attributes\n",
        "        self.create_temp_only_layers()\n",
        "\n",
        "        # input shape: (B * T) * (F * (1 + Y) + diagnosis_size + no_flat_features)\n",
        "        # output shape: (B * T) * last_linear_size\n",
        "        input_size = self.F * (1 + self.Y) + self.diagnosis_size + self.no_flat_features\n",
        "        self.point_last_los = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        self.point_last_mort = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        return\n",
        "\n",
        "\n",
        "    def init_pointwise(self):\n",
        "\n",
        "        # non-module layer attributes\n",
        "        self.layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            point_size = self.point_sizes[i]\n",
        "            self.update_layer_info(layer=i, point_size=point_size)\n",
        "\n",
        "        # module layer attributes\n",
        "        self.create_pointwise_only_layers()\n",
        "\n",
        "        # input shape: (B * T) * (Zt + 2F + 2 + no_flat_features + diagnosis_size)\n",
        "        # output shape: (B * T) * last_linear_size\n",
        "        if self.no_mask:\n",
        "            input_size = self.Zt + self.F + 2 + self.no_flat_features + self.diagnosis_size\n",
        "        else:\n",
        "            input_size = self.Zt + 2 * self.F + 2 + self.no_flat_features + self.diagnosis_size\n",
        "        self.point_last_los = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        self.point_last_mort = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def update_layer_info(self, layer=None, temp_k=None, point_size=None, dilation=None, stride=None):\n",
        "\n",
        "        self.layers.append({})\n",
        "        if point_size is not None:\n",
        "            self.layers[layer]['point_size'] = point_size\n",
        "        if temp_k is not None:\n",
        "            padding = [(self.kernel_size - 1) * dilation, 0]  # [padding_left, padding_right]\n",
        "            self.layers[layer]['temp_kernels'] = temp_k\n",
        "            self.layers[layer]['dilation'] = dilation\n",
        "            self.layers[layer]['padding'] = padding\n",
        "            self.layers[layer]['stride'] = stride\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def create_temp_pointwise_layers(self):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Z is the number of extra features added by the previous pointwise layer (could be 0 if this is the first layer)\n",
        "        # Zt is the cumulative number of extra features that have been added by all previous pointwise layers\n",
        "        # Zt-1 = Zt - Z (cumulative number of extra features minus the most recent pointwise layer)\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "\n",
        "        self.layer_modules = nn.ModuleDict()\n",
        "\n",
        "        self.Y = 0\n",
        "        self.Z = 0\n",
        "        self.Zt = 0\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            temp_in_channels = (self.F + self.Zt) * (1 + self.Y) if i > 0 else 2 * self.F  # (F + Zt) * (Y + 1)\n",
        "            temp_out_channels = (self.F + self.Zt) * self.layers[i]['temp_kernels']  # (F + Zt) * temp_kernels\n",
        "            linear_input_dim = (self.F + self.Zt - self.Z) * self.Y + self.Z + 2 * self.F + 2 + self.no_flat_features  # (F + Zt-1) * Y + Z + 2F + 2 + no_flat_features\n",
        "            linear_output_dim = self.layers[i]['point_size']  # point_size\n",
        "            # correct if no_mask\n",
        "            if self.no_mask:\n",
        "                if i == 0:\n",
        "                    temp_in_channels = self.F\n",
        "                linear_input_dim = (self.F + self.Zt - self.Z) * self.Y + self.Z + self.F + 2 + self.no_flat_features  # (F + Zt-1) * Y + Z + F + 2 + no_flat_features\n",
        "\n",
        "            temp = nn.Conv1d(in_channels=temp_in_channels,  # (F + Zt) * (Y + 1)\n",
        "                             out_channels=temp_out_channels,  # (F + Zt) * Y\n",
        "                             kernel_size=self.kernel_size,\n",
        "                             stride=self.layers[i]['stride'],\n",
        "                             dilation=self.layers[i]['dilation'],\n",
        "                             groups=self.F + self.Zt)\n",
        "\n",
        "            point = nn.Linear(in_features=linear_input_dim, out_features=linear_output_dim)\n",
        "\n",
        "            # correct if no_skip_connections\n",
        "            if self.no_skip_connections:\n",
        "                temp_in_channels = self.F * self.Y if i > 0 else 2 * self.F  # F * Y\n",
        "                temp_out_channels = self.F * self.layers[i]['temp_kernels']  # F * temp_kernels\n",
        "                #linear_input_dim = self.F * self.Y + self.Z if i > 0 else 2 * self.F + 2 + self.no_flat_features  # (F * Y) + Z\n",
        "                linear_input_dim = self.Z if i > 0 else 2 * self.F + 2 + self.no_flat_features  # Z\n",
        "                temp = nn.Conv1d(in_channels=temp_in_channels,\n",
        "                                 out_channels=temp_out_channels,\n",
        "                                 kernel_size=self.kernel_size,\n",
        "                                 stride=self.layers[i]['stride'],\n",
        "                                 dilation=self.layers[i]['dilation'],\n",
        "                                 groups=self.F)\n",
        "\n",
        "                point = nn.Linear(in_features=linear_input_dim, out_features=linear_output_dim)\n",
        "\n",
        "            if self.batchnorm in ['default', 'mybatchnorm', 'low_momentum']:\n",
        "                bn_temp = self.batchnormclass(num_features=temp_out_channels, momentum=self.momentum)\n",
        "                bn_point = self.batchnormclass(num_features=linear_output_dim, momentum=self.momentum)\n",
        "            elif self.batchnorm == 'temponly':\n",
        "                bn_temp = self.batchnormclass(num_features=temp_out_channels)\n",
        "                bn_point = self.empty_module\n",
        "            elif self.batchnorm == 'pointonly':\n",
        "                bn_temp = self.empty_module\n",
        "                bn_point = self.batchnormclass(num_features=linear_output_dim)\n",
        "            else:\n",
        "                bn_temp = bn_point = self.empty_module  # linear module; does nothing\n",
        "\n",
        "            self.layer_modules[str(i)] = nn.ModuleDict({\n",
        "                'temp': temp,\n",
        "                'bn_temp': bn_temp,\n",
        "                'point': point,\n",
        "                'bn_point': bn_point})\n",
        "\n",
        "            self.Y = self.layers[i]['temp_kernels']\n",
        "            self.Z = linear_output_dim\n",
        "            self.Zt += self.Z\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def create_temp_only_layers(self):\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        self.layer_modules = nn.ModuleDict()\n",
        "        self.Y = 0\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            if self.share_weights:\n",
        "                temp_in_channels = (1 + self.Y) if i > 0 else 2  # (Y + 1)\n",
        "                temp_out_channels = self.layers[i]['temp_kernels']\n",
        "                groups = 1\n",
        "            else:\n",
        "                temp_in_channels = self.F * (1 + self.Y) if i > 0 else 2 * self.F  # F * (Y + 1)\n",
        "                temp_out_channels = self.F * self.layers[i]['temp_kernels']  # F * temp_kernels\n",
        "                groups = self.F\n",
        "\n",
        "            temp = nn.Conv1d(in_channels=temp_in_channels,\n",
        "                             out_channels=temp_out_channels,\n",
        "                             kernel_size=self.kernel_size,\n",
        "                             stride=self.layers[i]['stride'],\n",
        "                             dilation=self.layers[i]['dilation'],\n",
        "                             groups=groups)\n",
        "\n",
        "            if self.batchnorm in ['default', 'mybatchnorm', 'low_momentum', 'temponly']:\n",
        "                bn_temp = self.batchnormclass(num_features=temp_out_channels, momentum=self.momentum)\n",
        "            else:\n",
        "                bn_temp = self.empty_module  # linear module; does nothing\n",
        "\n",
        "            self.layer_modules[str(i)] = nn.ModuleDict({\n",
        "                'temp': temp,\n",
        "                'bn_temp': bn_temp})\n",
        "\n",
        "            self.Y = self.layers[i]['temp_kernels']\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def create_pointwise_only_layers(self):\n",
        "\n",
        "        # Zt is the cumulative number of extra features that have been added by previous pointwise layers\n",
        "        self.layer_modules = nn.ModuleDict()\n",
        "        self.Zt = 0\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            linear_input_dim = self.Zt + 2 * self.F + 2 + self.no_flat_features  # Zt + 2F + 2 + no_flat_features\n",
        "            linear_output_dim = self.layers[i]['point_size']  # point_size\n",
        "\n",
        "            if self.no_mask:\n",
        "                linear_input_dim = self.Zt + self.F + 2 + self.no_flat_features  # Zt + 2F + 2 + no_flat_features\n",
        "\n",
        "            point = nn.Linear(in_features=linear_input_dim, out_features=linear_output_dim)\n",
        "\n",
        "            if self.batchnorm in ['default', 'mybatchnorm', 'low_momentum', 'pointonly']:\n",
        "                bn_point = self.batchnormclass(num_features=linear_output_dim, momentum=self.momentum)\n",
        "            else:\n",
        "                bn_point = self.empty_module  # linear module; does nothing\n",
        "\n",
        "            self.layer_modules[str(i)] = nn.ModuleDict({\n",
        "                'point': point,\n",
        "                'bn_point': bn_point})\n",
        "\n",
        "            self.Zt += linear_output_dim\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    # This is really where the crux of TPC is defined. This function defines one TPC layer, as in Figure 3 in the paper:\n",
        "    # https://arxiv.org/pdf/2007.09483.pdf\n",
        "    def temp_pointwise(self, B=None, T=None, X=None, repeat_flat=None, X_orig=None, temp=None, bn_temp=None, point=None,\n",
        "                       bn_point=None, temp_kernels=None, point_size=None, padding=None, prev_temp=None, prev_point=None,\n",
        "                       point_skip=None):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Z is the number of extra features added by the previous pointwise layer (could be 0 if this is the first layer)\n",
        "        # Zt is the cumulative number of extra features that have been added by all previous pointwise layers\n",
        "        # Zt-1 = Zt - Z (cumulative number of extra features minus the most recent pointwise layer)\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # X shape: B * ((F + Zt) * (Y + 1)) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "        # repeat_flat shape: (B * T) * no_flat_features\n",
        "        # X_orig shape: (B * T) * (2F + 2)\n",
        "        # prev_temp shape: (B * T) * ((F + Zt-1) * (Y + 1))\n",
        "        # prev_point shape: (B * T) * Z\n",
        "\n",
        "        Z = prev_point.shape[1] if prev_point is not None else 0\n",
        "\n",
        "        X_padded = pad(X, padding, 'constant', 0)  # B * ((F + Zt) * (Y + 1)) * (T + padding)\n",
        "        X_temp = self.temp_dropout(bn_temp(temp(X_padded)))  # B * ((F + Zt) * temp_kernels) * T\n",
        "\n",
        "        X_concat = cat(self.remove_none((prev_temp,  # (B * T) * ((F + Zt-1) * Y)\n",
        "                                         prev_point,  # (B * T) * Z\n",
        "                                         X_orig,  # (B * T) * (2F + 2)\n",
        "                                         repeat_flat)),  # (B * T) * no_flat_features\n",
        "                       dim=1)  # (B * T) * (((F + Zt-1) * Y) + Z + 2F + 2 + no_flat_features)\n",
        "\n",
        "        point_output = self.main_dropout(bn_point(point(X_concat)))  # (B * T) * point_size\n",
        "\n",
        "        # point_skip input: B * (F + Zt-1) * T\n",
        "        # prev_point: B * Z * T\n",
        "        # point_skip output: B * (F + Zt) * T\n",
        "        point_skip = cat((point_skip, prev_point.view(B, T, Z).permute(0, 2, 1)), dim=1) if prev_point is not None else point_skip\n",
        "\n",
        "        temp_skip = cat((point_skip.unsqueeze(2),  # B * (F + Zt) * 1 * T\n",
        "                         X_temp.view(B, point_skip.shape[1], temp_kernels, T)),  # B * (F + Zt) * temp_kernels * T\n",
        "                        dim=2)  # B * (F + Zt) * (1 + temp_kernels) * T\n",
        "\n",
        "        X_point_rep = point_output.view(B, T, point_size, 1).permute(0, 2, 3, 1).repeat(1, 1, (1 + temp_kernels), 1)  # B * point_size * (1 + temp_kernels) * T\n",
        "        X_combined = self.relu(cat((temp_skip, X_point_rep), dim=1))  # B * (F + Zt) * (1 + temp_kernels) * T\n",
        "        next_X = X_combined.view(B, (point_skip.shape[1] + point_size) * (1 + temp_kernels), T)  # B * ((F + Zt + point_size) * (1 + temp_kernels)) * T\n",
        "\n",
        "        temp_output = X_temp.permute(0, 2, 1).contiguous().view(B * T, point_skip.shape[1] * temp_kernels)  # (B * T) * ((F + Zt) * temp_kernels)\n",
        "\n",
        "        return (temp_output,  # (B * T) * ((F + Zt) * temp_kernels)\n",
        "                point_output,  # (B * T) * point_size\n",
        "                next_X,  # B * ((F + Zt) * (1 + temp_kernels)) * T\n",
        "                point_skip)  # for keeping track of the point skip connections; B * (F + Zt) * T\n",
        "\n",
        "\n",
        "    def temp(self, B=None, T=None, X=None, X_temp_orig=None, temp=None, bn_temp=None, temp_kernels=None, padding=None):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # X shape: B * (F * (Y + 1)) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "        # X_temp_orig shape: B * F * T\n",
        "\n",
        "        X_padded = pad(X, padding, 'constant', 0)  # B * (F * (Y + 1)) * (T + padding)\n",
        "\n",
        "        if self.share_weights:\n",
        "            _, C, padded_length = X_padded.shape\n",
        "            chans = int(C / self.F)\n",
        "            X_temp = self.temp_dropout(bn_temp(temp(X_padded.view(B * self.F, chans, padded_length)))).view(B, (self.F * temp_kernels), T)  # B * (F * temp_kernels) * T\n",
        "        else:\n",
        "            X_temp = self.temp_dropout(bn_temp(temp(X_padded)))  # B * (F * temp_kernels) * T\n",
        "\n",
        "        temp_skip = self.relu(cat((X_temp_orig.unsqueeze(2),  # B * F * 1 * T\n",
        "                                   X_temp.view(B, self.F, temp_kernels, T)),  # B * F * temp_kernels * T\n",
        "                                   dim=2))  # B * F * (1 + temp_kernels) * T\n",
        "\n",
        "        next_X = temp_skip.view(B, (self.F * (1 + temp_kernels)), T)  # B * (F * (1 + temp_kernels)) * T\n",
        "\n",
        "        return next_X  # B * (F * temp_kernels) * T\n",
        "\n",
        "\n",
        "    def point(self, B=None, T=None, X=None, repeat_flat=None, X_orig=None, point=None, bn_point=None, point_skip=None):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Z is the number of extra features added by the previous pointwise layer (could be 0 if this is the first layer)\n",
        "        # Zt is the cumulative number of extra features that have been added by all previous pointwise layers\n",
        "        # Zt-1 = Zt - Z (cumulative number of extra features minus the most recent pointwise layer)\n",
        "        # X shape: B * (F + Zt) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "        # repeat_flat shape: (B * T) * no_flat_features\n",
        "        # X_orig shape: (B * T) * (2F + 2)\n",
        "        # prev_point shape: (B * T) * Z\n",
        "\n",
        "        X_combined = cat((X, repeat_flat), dim=1)\n",
        "\n",
        "        X_point = self.main_dropout(bn_point(point(X_combined)))  # (B * T) * point_size\n",
        "\n",
        "        # point_skip input: B * Zt-1 * T\n",
        "        # prev_point: B * Z * T\n",
        "        # point_skip output: B * Zt * T\n",
        "        point_skip = cat(self.remove_none((point_skip, X_point.view(B, T, -1).permute(0, 2, 1))), dim=1)\n",
        "\n",
        "        # point_skip: B * Zt * T\n",
        "        # X_orig: (B * T) * (2F + 2)\n",
        "        # repeat_flat: (B * T) * no_flat_features\n",
        "        # next_X: (B * T) * (Zt + 2F + 2 + no_flat_features)\n",
        "        next_X = self.relu(cat((point_skip.permute(0, 2, 1).contiguous().view(B * T, -1), X_orig), dim=1))\n",
        "\n",
        "        return (next_X,  # (B * T) * (Zt + 2F + 2 + no_flat_features)\n",
        "                point_skip)  # for keeping track of the pointwise skip connections; B * Zt * T\n",
        "\n",
        "\n",
        "    def temp_pointwise_no_skip(self, B=None, T=None, temp=None, bn_temp=None, point=None, bn_point=None, padding=None, prev_temp=None,\n",
        "                               prev_point=None, temp_kernels=None, X_orig=None, repeat_flat=None):\n",
        "\n",
        "        ### Temporal component ###\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # prev_temp shape: B * (F * Y) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "\n",
        "        X_padded = pad(prev_temp, padding, 'constant', 0)  # B * (F * Y) * (T + padding)\n",
        "        temp_output = self.relu(self.temp_dropout(bn_temp(temp(X_padded))))  # B * (F * temp_kernels) * T\n",
        "\n",
        "        ### Pointwise component ###\n",
        "\n",
        "        # prev_point shape: (B * T) * ((F * Y) + Z)\n",
        "        point_output = self.relu(self.main_dropout(bn_point(point(prev_point))))  # (B * T) * point_size\n",
        "\n",
        "        return (temp_output,  # B * (F * temp_kernels) * T\n",
        "                point_output)  # (B * T) * point_size\n",
        "\n",
        "\n",
        "    def forward(self, X, diagnoses, flat, time_before_pred=5):\n",
        "\n",
        "        # flat is B * no_flat_features\n",
        "        # diagnoses is B * D\n",
        "        # X is B * (2F + 2) * T\n",
        "        # X_mask is B * T\n",
        "        # (the batch is padded to the longest sequence, the + 2 is the time and the hour which are not for temporal convolution)\n",
        "\n",
        "        # get rid of the time and hour fields - these shouldn't go through the temporal network\n",
        "        # and split into features and indicator variables\n",
        "        X_separated = torch.split(X[:, 1:-1, :], self.F, dim=1)  # tuple ((B * F * T), (B * F * T))\n",
        "\n",
        "        # prepare repeat arguments and initialise layer loop\n",
        "        B, _, T = X_separated[0].shape\n",
        "        if self.model_type in ['pointwise_only', 'tpc']:\n",
        "            repeat_flat = flat.repeat_interleave(T, dim=0)  # (B * T) * no_flat_features\n",
        "            if self.no_mask:\n",
        "                X_orig = cat((X_separated[0],\n",
        "                              X[:, 0, :].unsqueeze(1),\n",
        "                              X[:, -1, :].unsqueeze(1)), dim=1).permute(0, 2, 1).contiguous().view(B * T, self.F + 2)  # (B * T) * (F + 2)\n",
        "            else:\n",
        "                X_orig = X.permute(0, 2, 1).contiguous().view(B * T, 2 * self.F + 2)  # (B * T) * (2F + 2)\n",
        "            repeat_args = {'repeat_flat': repeat_flat,\n",
        "                           'X_orig': X_orig,\n",
        "                           'B': B,\n",
        "                           'T': T}\n",
        "            if self.model_type == 'tpc':\n",
        "                if self.no_mask:\n",
        "                    next_X = X_separated[0]\n",
        "                else:\n",
        "                    next_X = torch.stack(X_separated, dim=2).reshape(B, 2 * self.F, T)  # B * 2F * T\n",
        "                point_skip = X_separated[0]  # keeps track of skip connections generated from linear layers; B * F * T\n",
        "                temp_output = None\n",
        "                point_output = None\n",
        "            else:  # pointwise only\n",
        "                next_X = X_orig\n",
        "                point_skip = None\n",
        "        elif self.model_type == 'temp_only':\n",
        "            next_X = torch.stack(X_separated, dim=2).view(B, 2 * self.F, T)  # B * 2F * T\n",
        "            X_temp_orig = X_separated[0]  # skip connections for temp only model\n",
        "            repeat_args = {'X_temp_orig': X_temp_orig,\n",
        "                           'B': B,\n",
        "                           'T': T}\n",
        "\n",
        "        if self.no_skip_connections:\n",
        "            temp_output = next_X\n",
        "            point_output = cat((X_orig,  # (B * T) * (2F + 2)\n",
        "                                repeat_flat),  # (B * T) * no_flat_features\n",
        "                               dim=1)  # (B * T) * (2F + 2 + no_flat_features)\n",
        "            self.layer1 = True\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            kwargs = dict(self.layer_modules[str(i)], **repeat_args)\n",
        "            if self.model_type == 'tpc':\n",
        "                if self.no_skip_connections:\n",
        "                    temp_output, point_output = self.temp_pointwise_no_skip(prev_point=point_output, prev_temp=temp_output,\n",
        "                                                                            temp_kernels=self.layers[i]['temp_kernels'],\n",
        "                                                                            padding=self.layers[i]['padding'], **kwargs)\n",
        "\n",
        "                else:\n",
        "                    temp_output, point_output, next_X, point_skip = self.temp_pointwise(X=next_X, point_skip=point_skip,\n",
        "                                                                        prev_temp=temp_output, prev_point=point_output,\n",
        "                                                                        temp_kernels=self.layers[i]['temp_kernels'],\n",
        "                                                                        padding=self.layers[i]['padding'],\n",
        "                                                                        point_size=self.layers[i]['point_size'],\n",
        "                                                                        **kwargs)\n",
        "            elif self.model_type == 'temp_only':\n",
        "                next_X = self.temp(X=next_X, temp_kernels=self.layers[i]['temp_kernels'],\n",
        "                                   padding=self.layers[i]['padding'], **kwargs)\n",
        "            elif self.model_type == 'pointwise_only':\n",
        "                next_X, point_skip = self.point(X=next_X, point_skip=point_skip, **kwargs)\n",
        "\n",
        "        # tidy up\n",
        "        if self.model_type == 'pointwise_only':\n",
        "            next_X = next_X.view(B, T, -1).permute(0, 2, 1)\n",
        "        elif self.no_skip_connections:\n",
        "            # combine the final layer\n",
        "            next_X = cat((point_output,\n",
        "                          temp_output.permute(0, 2, 1).contiguous().view(B * T, self.F * self.layers[-1]['temp_kernels'])),\n",
        "                         dim=1)\n",
        "            next_X = next_X.view(B, T, -1).permute(0, 2, 1)\n",
        "\n",
        "        # note that we cut off at time_before_pred hours here because the model is only valid from time_before_pred hours onwards\n",
        "        if self.no_diag:\n",
        "            combined_features = cat((flat.repeat_interleave(T - time_before_pred, dim=0),  # (B * (T - time_before_pred)) * no_flat_features\n",
        "                                     next_X[:, :, time_before_pred:].permute(0, 2, 1).contiguous().view(B * (T - time_before_pred), -1)), dim=1)  # (B * (T - time_before_pred)) * (((F + Zt) * (1 + Y)) + no_flat_features) for tpc\n",
        "        else:\n",
        "            diagnoses_enc = self.relu(self.main_dropout(self.bn_diagnosis_encoder(self.diagnosis_encoder(diagnoses))))  # B * diagnosis_size\n",
        "            combined_features = cat((flat.repeat_interleave(T - time_before_pred, dim=0),  # (B * (T - time_before_pred)) * no_flat_features\n",
        "                                     diagnoses_enc.repeat_interleave(T - time_before_pred, dim=0),  # (B * (T - time_before_pred)) * diagnosis_size\n",
        "                                     next_X[:, :, time_before_pred:].permute(0, 2, 1).contiguous().view(B * (T - time_before_pred), -1)), dim=1)  # (B * (T - time_before_pred)) * (((F + Zt) * (1 + Y)) + diagnosis_size + no_flat_features) for tpc\n",
        "\n",
        "        last_point_los = self.relu(self.main_dropout(self.bn_point_last_los(self.point_last_los(combined_features))))\n",
        "        last_point_mort = self.relu(self.main_dropout(self.bn_point_last_mort(self.point_last_mort(combined_features))))\n",
        "\n",
        "        if self.no_exp:\n",
        "            los_predictions = self.hardtanh(self.point_final_los(last_point_los).view(B, T - time_before_pred))  # B * (T - time_before_pred)\n",
        "        else:\n",
        "            los_predictions = self.hardtanh(exp(self.point_final_los(last_point_los).view(B, T - time_before_pred)))  # B * (T - time_before_pred)\n",
        "        mort_predictions = self.sigmoid(self.point_final_mort(last_point_mort).view(B, T - time_before_pred))  # B * (T - time_before_pred)\n",
        "\n",
        "        return los_predictions, mort_predictions\n",
        "\n",
        "\n",
        "    def temp_pointwise_no_skip_old(self, B=None, T=None, temp=None, bn_temp=None, point=None, bn_point=None, padding=None, prev_temp=None,\n",
        "                               prev_point=None, temp_kernels=None, X_orig=None, repeat_flat=None):\n",
        "\n",
        "        ### Temporal component ###\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # prev_temp shape: B * (F * Y) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "\n",
        "        X_padded = pad(prev_temp, padding, 'constant', 0)  # B * (F * Y) * (T + padding)\n",
        "        temp_output = self.relu(self.temp_dropout(bn_temp(temp(X_padded))))  # B * (F * temp_kernels) * T\n",
        "\n",
        "        ### Pointwise component ###\n",
        "\n",
        "        # prev_point shape: (B * T) * ((F * Y) + Z)\n",
        "\n",
        "        # if this is not layer 1:\n",
        "        if self.layer1:\n",
        "            X_concat = prev_point\n",
        "            self.layer1 = False\n",
        "        else:\n",
        "            X_concat = cat((prev_point,\n",
        "                            prev_temp.permute(0, 2, 1).contiguous().view(B * T, self.F * temp_kernels)),\n",
        "                           dim=1)\n",
        "\n",
        "        point_output = self.relu(self.main_dropout(bn_point(point(X_concat))))  # (B * T) * point_size\n",
        "\n",
        "        return (temp_output,  # B * (F * temp_kernels) * T\n",
        "                point_output)  # (B * T) * point_size\n",
        "\n",
        "\n",
        "    def loss(self, y_hat_los, y_hat_mort, y_los, y_mort, mask, seq_lengths, device, sum_losses, loss_type):\n",
        "        # mort loss\n",
        "        if self.task == 'mortality':\n",
        "            loss = self.bce_loss(y_hat_mort, y_mort) * self.alpha\n",
        "        # los loss\n",
        "        else:\n",
        "            bool_type = torch.cuda.BoolTensor if device == torch.device('cuda') else torch.BoolTensor\n",
        "            if loss_type == 'msle':\n",
        "                los_loss = self.msle_loss(y_hat_los, y_los, mask.type(bool_type), seq_lengths, sum_losses)\n",
        "            elif loss_type == 'mse':\n",
        "                los_loss = self.mse_loss(y_hat_los, y_los, mask.type(bool_type), seq_lengths, sum_losses)\n",
        "            if self.task == 'LoS':\n",
        "                loss = los_loss\n",
        "            # multitask loss\n",
        "            if self.task == 'multitask':\n",
        "                loss = los_loss + self.bce_loss(y_hat_mort, y_mort) * self.alpha\n",
        "        return loss"
      ],
      "metadata": {
        "id": "yKPGdmaPr6Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Training\n",
        "Computational requirements:\n",
        "The model needs a GPU with high memory bandwidth for faster processing and training. Tools like Conda or Docker can help with dependencies for dev enviorments."
      ],
      "metadata": {
        "id": "j_jHz8Fk1Kf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class EICUDataset(Dataset):\n",
        "    def __init__(self, root_dir, mode='train'):\n",
        "        \"\"\"\n",
        "        root_dir (string): Directory with all the data files.\n",
        "        mode (string): One of 'train', 'val', or 'test' to specify the subset of data.\n",
        "        \"\"\"\n",
        "        self.flat = pd.read_csv(f\"{root_dir}/{mode}/flat.csv\")\n",
        "        self.labels = pd.read_csv(f\"{root_dir}/{mode}/labels.csv\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.flat)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Assuming your labels.csv has a binary classification target\n",
        "        labels = self.labels.iloc[idx, 1]  # Modify the index if necessary\n",
        "        features = self.flat.iloc[idx, 1:].values  # Assuming the first column is not a feature\n",
        "\n",
        "        # Convert to tensor\n",
        "        features = torch.from_numpy(features).float()\n",
        "        labels = torch.tensor(labels).float()\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "train_dataset = EICUDataset(raw_data_dir, mode='train')\n",
        "val_dataset = EICUDataset(raw_data_dir, mode='val')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "config = {\n",
        "    'task': 'LoS',\n",
        "    'n_layers': 3,\n",
        "    'model_type': 'tpc',\n",
        "    'share_weights': False,\n",
        "    'diagnosis_size': 100,\n",
        "    'main_dropout_rate': 0.1,\n",
        "    'temp_dropout_rate': 0.1,\n",
        "    'kernel_size': 3,\n",
        "    'temp_kernels': [10, 20, 30],  # Example sizes\n",
        "    'point_sizes': [100, 100, 100],\n",
        "    'batchnorm': 'default',\n",
        "    'last_linear_size': 50,\n",
        "    'F': 50,  # Feature count\n",
        "    'D': 10,  # Diagnosis vector size\n",
        "    'no_flat_features': 15,\n",
        "    'no_diag': False,\n",
        "    'no_mask': False,\n",
        "    'no_exp': False,\n",
        "    'no_skip_connections': False,\n",
        "    'alpha': 0.5,  # Example for multitask weighting\n",
        "    'momentum': 0.1\n",
        "}\n",
        "model = TempPointConv(config=config)\n",
        "loss_func = MSLELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "def train_model_one_iter(model, loss_func, optimizer, train_loader, device):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  total_samples = 0\n",
        "  for batch_idx, batch in enumerate(train_loader):\n",
        "      if batch_idx > (config.no_train_batches // (100 / config.percentage_data)):\n",
        "          break\n",
        "\n",
        "      if config.dataset == 'MIMIC':\n",
        "          padded, mask, flat, los_labels, mort_labels, seq_lengths = batch\n",
        "          diagnoses = None\n",
        "      else:\n",
        "          padded, mask, diagnoses, flat, los_labels, mort_labels, seq_lengths = batch\n",
        "\n",
        "      padded, mask, flat = padded.to(device), mask.to(device), flat.to(device)\n",
        "      if diagnoses is not None:\n",
        "          diagnoses = diagnoses.to(device)\n",
        "      los_labels, mort_labels = los_labels.to(device), mort_labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      y_hat_los, y_hat_mort = model(padded, diagnoses, flat)\n",
        "      loss = model.loss(y_hat_los, y_hat_mort, los_labels, mort_labels, mask, seq_lengths, device, config.sum_losses, config.loss_type)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss.append(loss.item() * padded.size(0))\n",
        "      total_samples += padded.size(0)\n",
        "\n",
        "      if config.intermediate_reporting and batch_idx % config.log_interval == 0 and batch_idx != 0:\n",
        "          mean_loss = sum(total_loss[-config.log_interval:]) / min(config.log_interval, len(total_loss))\n",
        "          print(f'Intermediate Training Loss after {batch_idx} batches: {mean_loss:.4f}')\n",
        "\n",
        "  mean_train_loss = sum(total_loss) / total_samples\n",
        "  return mean_train_loss\n",
        "\n",
        "\n",
        "def validate_model(model, valid_loader, device):\n",
        "  model.eval()\n",
        "  val_losses = []\n",
        "  predictions, actuals = [], []\n",
        "  with torch.no_grad():\n",
        "      for features, labels in valid_loader:\n",
        "          features, labels = features.to(device), labels.to(device)\n",
        "          outputs = model(features)\n",
        "          loss = loss_func(outputs, labels)\n",
        "          val_losses.append(loss.item() * features.size(0))\n",
        "          predictions.extend(outputs.cpu().numpy())\n",
        "          actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "  val_loss = np.sum(val_losses) / np.sum([x.size(0) for _, x in valid_loader])\n",
        "  metrics = print_metrics_regression(np.array(actuals), np.array(predictions))\n",
        "  return val_loss, metrics\n"
      ],
      "metadata": {
        "id": "SDGuapzS1J6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Evaluation\n",
        "Metrics Description\n",
        "* Mean Absolute Percentage Error (MAPE): Measures the average magnitude of the errors in a set of predictions\n",
        "* Mean Squared Logarithmic Error (MSLE): Similar to Mean Squared Error, but it first takes the logarithm of the predictions and actual values, which can dampen the effect of large outliers.\n",
        "* Mean Absolute Deviation (MAD): Represents the average absolute difference between the actual and predicted values.\n",
        "* Mean Squared Error (MSE): Represents the average squared difference between the estimated values and the actual value.\n",
        "* R² Score: Provides an indication of goodness of fit and measures how well unseen samples are likely to be predicted by the model, through the proportion of variance explained.\n",
        "* Cohen’s Kappa: Measures the agreement between the predicted and actual categorizations."
      ],
      "metadata": {
        "id": "MT_-05_J1SCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#These are metrics that will be used to evaluation function\n",
        "from sklearn import metrics\n",
        "\n",
        "class CustomBins:\n",
        "    inf = 1e18\n",
        "    bins = [(-inf, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 14), (14, +inf)]\n",
        "    nbins = len(bins)\n",
        "\n",
        "def get_bin_custom(x, nbins, one_hot=False):\n",
        "    for i in range(nbins):\n",
        "        a = CustomBins.bins[i][0]\n",
        "        b = CustomBins.bins[i][1]\n",
        "        if a <= x < b:\n",
        "            if one_hot:\n",
        "                onehot = np.zeros((CustomBins.nbins,))\n",
        "                onehot[i] = 1\n",
        "                return onehot\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(4/24, y_true))) * 100  # this stops the mape being a stupidly large value when y_true happens to be very small\n",
        "\n",
        "def mean_squared_logarithmic_error(y_true, y_pred):\n",
        "    return np.mean(np.square(np.log(y_true/y_pred)))\n",
        "\n",
        "def print_metrics_regression(y_true, predictions, verbose=1, elog=None):\n",
        "    print('==> Length of Stay:')\n",
        "    y_true_bins = [get_bin_custom(x, CustomBins.nbins) for x in y_true]\n",
        "    prediction_bins = [get_bin_custom(x, CustomBins.nbins) for x in predictions]\n",
        "    cf = metrics.confusion_matrix(y_true_bins, prediction_bins)\n",
        "    if elog is not None:\n",
        "        elog.print('Custom bins confusion matrix:')\n",
        "        elog.print(cf)\n",
        "    elif verbose:\n",
        "        print('Custom bins confusion matrix:')\n",
        "        print(cf)\n",
        "\n",
        "    kappa = metrics.cohen_kappa_score(y_true_bins, prediction_bins, weights='linear')\n",
        "    mad = metrics.mean_absolute_error(y_true, predictions)\n",
        "    mse = metrics.mean_squared_error(y_true, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_true, predictions)\n",
        "    msle = mean_squared_logarithmic_error(y_true, predictions)\n",
        "    r2 = metrics.r2_score(y_true, predictions)\n",
        "\n",
        "    if verbose:\n",
        "        print('Mean absolute deviation (MAD) = {}'.format(mad))\n",
        "        print('Mean squared error (MSE) = {}'.format(mse))\n",
        "        print('Mean absolute percentage error (MAPE) = {}'.format(mape))\n",
        "        print('Mean squared logarithmic error (MSLE) = {}'.format(msle))\n",
        "        print('R^2 Score = {}'.format(r2))\n",
        "        print('Cohen kappa score = {}'.format(kappa))\n",
        "\n",
        "    return [mad, mse, mape, msle, r2, kappa]\n",
        "\n",
        "# def print_metrics_mortality(y_true, prediction_probs, verbose=1, elog=None):\n",
        "#     print('==> Mortality:')\n",
        "#     prediction_probs = np.array(prediction_probs)\n",
        "#     prediction_probs = np.transpose(np.append([1 - prediction_probs], [prediction_probs], axis=0))\n",
        "#     predictions = prediction_probs.argmax(axis=1)\n",
        "#     cf = metrics.confusion_matrix(y_true, predictions, labels=range(2))\n",
        "#     if elog is not None:\n",
        "#         elog.print('Confusion matrix:')\n",
        "#         elog.print(cf)\n",
        "#     elif verbose:\n",
        "#         print('Confusion matrix:')\n",
        "#         print(cf)\n",
        "#     cf = cf.astype(np.float32)\n",
        "\n",
        "#     acc = (cf[0][0] + cf[1][1]) / np.sum(cf)\n",
        "#     prec0 = cf[0][0] / (cf[0][0] + cf[1][0])\n",
        "#     prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
        "#     rec0 = cf[0][0] / (cf[0][0] + cf[0][1])\n",
        "#     rec1 = cf[1][1] / (cf[1][1] + cf[1][0])\n",
        "\n",
        "#     auroc = metrics.roc_auc_score(y_true, prediction_probs[:, 1])\n",
        "#     (precisions, recalls, thresholds) = metrics.precision_recall_curve(y_true, prediction_probs[:, 1])\n",
        "#     auprc = metrics.auc(recalls, precisions)\n",
        "#     f1macro = metrics.f1_score(y_true, predictions, average='macro')\n",
        "\n",
        "#     results = {'Accuracy': acc, 'Precision Survived': prec0, 'Precision Died': prec1, 'Recall Survived': rec0,\n",
        "#                'Recall Died': rec1, 'Area Under the Receiver Operating Characteristic curve (AUROC)': auroc,\n",
        "#                'Area Under the Precision Recall curve (AUPRC)': auprc, 'F1 score (macro averaged)': f1macro}\n",
        "#     if verbose:\n",
        "#         for key in results:\n",
        "#             print('{} = {}'.format(key, results[key]))\n",
        "\n",
        "#     return [acc, prec0, prec1, rec0, rec1, auroc, auprc, f1macro]"
      ],
      "metadata": {
        "id": "OyG6glBo1Z73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "for epoch in range(num_epoch):\n",
        "    train_loss = train_model_one_iter(model, loss_func, optimizer, train_loader, device)\n",
        "    val_loss, metrics = validate_model(model, valid_loader, device)\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "    print(\"Validation Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "3ccqAw31ExMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results\n",
        "* The primary results from our experiments focused on predicting the Length of Stay (LoS) in medical care facilities using a Temporal Pointwise Convolution (TPC) model. The final experiment settings were tuned to optimize performance specifically for the LoS task. Detailed model performance was tracked using various metrics such as Mean Absolute Deviation (MAD), Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE), Mean Squared Logarithmic Error (MSLE), R^2 Score, and the Cohen Kappa Score. These metrics provide a comprehensive understanding of the model's accuracy and reliability in predicting patient stay durations.\n",
        "\n",
        "Analyses (have to add charts next)\n",
        "* Analysis indicated correlations between the model's predictions and actual outcomes, suggesting that the TPC model effectively captures the temporal dynamics and complexities associated with patient stays. The confusion matrix for custom bins, which categorizes stays into predefined duration intervals, further validated the model’s classification ability across different stay lengths.\n",
        "\n",
        "Plans\n",
        "* Add hyperparameter tuning more meticulousely so the best parameters are adopted.\n",
        "* Possibly analyze other predictive metrics like mortality rate that tpc model can capture.\n",
        "* Test two differnt ablations relating to loss function and input features.\n",
        "* Compare this model analysis to other transformer and baseline model metrics."
      ],
      "metadata": {
        "id": "otve3q2oJdIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# metrics to evaluate my model (calculated in eval step)\n",
        "# Mean absolute deviation (MAD)\n",
        "# Mean absolute percentage error (MAPE)\n",
        "# Mean squared error (MSE)\n",
        "# Mean squared log error (MSLE)\n",
        "# Coefficient of determination (R2)\n",
        "\n",
        "# plot figures to better show the results (will add more later)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss per Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Rocheteau, Emma, et al. “Temporal pointwise convolutional networks for length of stay prediction in the Intensive Care Unit.” Proceedings of the Conference on Health, Inference, and Learning, 8 Apr. 2021, https://doi.org/10.1145/3450439.3451860\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}